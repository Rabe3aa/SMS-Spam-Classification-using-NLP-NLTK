{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/Self studying/NLP/LinkedIn/Essential Training NLP/archive/SMSSpamCollection.tsv', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['label', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    text = text.lower()\n",
    "    temp = ''.join([i for i in text if i not in string.punctuation])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_punc'] = df['text'].apply(lambda x: remove_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive been searching for the right words to than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                           text_punc  \n",
       "0  ive been searching for the right words to than...  \n",
       "1  free entry in 2 a wkly comp to win fa cup fina...  \n",
       "2  nah i dont think he goes to usf he lives aroun...  \n",
       "3  even my brother is not like to speak with me t...  \n",
       "4                  i have a date on sunday with will  "
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    temp = re.split('\\W+', text)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_tokenized'] = df['text_punc'].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_punc</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive been searching for the right words to than...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                           text_punc  \\\n",
       "0  ive been searching for the right words to than...   \n",
       "1  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "2  nah i dont think he goes to usf he lives aroun...   \n",
       "3  even my brother is not like to speak with me t...   \n",
       "4                  i have a date on sunday with will   \n",
       "\n",
       "                                      text_tokenized  \n",
       "0  [ive, been, searching, for, the, right, words,...  \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, f...  \n",
       "2  [nah, i, dont, think, he, goes, to, usf, he, l...  \n",
       "3  [even, my, brother, is, not, like, to, speak, ...  \n",
       "4         [i, have, a, date, on, sunday, with, will]  "
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    sw = stopwords.words('english')\n",
    "    temp = [i for i in text if i not in sw]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_nonSW'] = df['text_tokenized'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_punc</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_nonSW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive been searching for the right words to than...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                           text_punc  \\\n",
       "0  ive been searching for the right words to than...   \n",
       "1  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "2  nah i dont think he goes to usf he lives aroun...   \n",
       "3  even my brother is not like to speak with me t...   \n",
       "4                  i have a date on sunday with will   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words,...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "2  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                          text_nonSW  \n",
       "0  [ive, searching, right, words, thank, breather...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "2  [nah, dont, think, goes, usf, lives, around, t...  \n",
       "3  [even, brother, like, speak, treat, like, aids...  \n",
       "4                                     [date, sunday]  "
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = WordNetLemmatizer()\n",
    "def Lemmatizing(text):\n",
    "    temp = [wn.lemmatize(i) for i in text]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_lemmatized'] = df['text_nonSW'].apply(lambda x: Lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_punc</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_nonSW</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive been searching for the right words to than...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather...</td>\n",
       "      <td>[ive, searching, right, word, thank, breather,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                           text_punc  \\\n",
       "0  ive been searching for the right words to than...   \n",
       "1  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "2  nah i dont think he goes to usf he lives aroun...   \n",
       "3  even my brother is not like to speak with me t...   \n",
       "4                  i have a date on sunday with will   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words,...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "2  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                          text_nonSW  \\\n",
       "0  [ive, searching, right, words, thank, breather...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, goes, usf, lives, around, t...   \n",
       "3  [even, brother, like, speak, treat, like, aids...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                     text_lemmatized  \n",
       "0  [ive, searching, right, word, thank, breather,...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "2  [nah, dont, think, go, usf, life, around, though]  \n",
       "3  [even, brother, like, speak, treat, like, aid,...  \n",
       "4                                     [date, sunday]  "
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr_to_str(text):\n",
    "    temp = ' '.join([i for i in text])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text_lemmatized'].apply(lambda x: arr_to_str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_punc</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_nonSW</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive been searching for the right words to than...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather...</td>\n",
       "      <td>[ive, searching, right, word, thank, breather,...</td>\n",
       "      <td>ive searching right word thank breather promis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid,...</td>\n",
       "      <td>even brother like speak treat like aid patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>date sunday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                           text_punc  \\\n",
       "0  ive been searching for the right words to than...   \n",
       "1  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "2  nah i dont think he goes to usf he lives aroun...   \n",
       "3  even my brother is not like to speak with me t...   \n",
       "4                  i have a date on sunday with will   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words,...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "2  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                          text_nonSW  \\\n",
       "0  [ive, searching, right, words, thank, breather...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, goes, usf, lives, around, t...   \n",
       "3  [even, brother, like, speak, treat, like, aids...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                     text_lemmatized  \\\n",
       "0  [ive, searching, right, word, thank, breather,...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, go, usf, life, around, though]   \n",
       "3  [even, brother, like, speak, treat, like, aid,...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                          clean_text  \n",
       "0  ive searching right word thank breather promis...  \n",
       "1  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "2           nah dont think go usf life around though  \n",
       "3      even brother like speak treat like aid patent  \n",
       "4                                        date sunday  "
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF - IDF Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_Tf_idf = tf_idf_vec.fit_transform(df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Tf_idf = pd.DataFrame(x_Tf_idf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Tf_idf.columns = tf_idf_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089my</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>02070836089</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8880 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   008704050406  0089my  0121  01223585236  01223585334  0125698789   02  \\\n",
       "0           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "1           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "2           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "3           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "4           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "\n",
       "   020603  0207  02070836089 ...   zero  zhong  zindgi  zoe  zogtorius  zoom  \\\n",
       "0     0.0   0.0          0.0 ...    0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "1     0.0   0.0          0.0 ...    0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "2     0.0   0.0          0.0 ...    0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "3     0.0   0.0          0.0 ...    0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "4     0.0   0.0          0.0 ...    0.0    0.0     0.0  0.0        0.0   0.0   \n",
       "\n",
       "   zouk  zyada  üll  〨ud  \n",
       "0   0.0    0.0  0.0  0.0  \n",
       "1   0.0    0.0  0.0  0.0  \n",
       "2   0.0    0.0  0.0  0.0  \n",
       "3   0.0    0.0  0.0  0.0  \n",
       "4   0.0    0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 8880 columns]"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Tf_idf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Tf_idf = pd.concat([df['len_text'], df['punc_per'],df_Tf_idf], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_punc</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_nonSW</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive been searching for the right words to than...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather...</td>\n",
       "      <td>[ive, searching, right, word, thank, breather,...</td>\n",
       "      <td>ive searching right word thank breather promis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid,...</td>\n",
       "      <td>even brother like speak treat like aid patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>date sunday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                           text_punc  \\\n",
       "0  ive been searching for the right words to than...   \n",
       "1  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "2  nah i dont think he goes to usf he lives aroun...   \n",
       "3  even my brother is not like to speak with me t...   \n",
       "4                  i have a date on sunday with will   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words,...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "2  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                          text_nonSW  \\\n",
       "0  [ive, searching, right, words, thank, breather...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, goes, usf, lives, around, t...   \n",
       "3  [even, brother, like, speak, treat, like, aids...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                     text_lemmatized  \\\n",
       "0  [ive, searching, right, word, thank, breather,...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, go, usf, life, around, though]   \n",
       "3  [even, brother, like, speak, treat, like, aid,...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                          clean_text  \n",
       "0  ive searching right word thank breather promis...  \n",
       "1  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "2           nah dont think go usf life around though  \n",
       "3      even brother like speak treat like aid patent  \n",
       "4                                        date sunday  "
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_text'] = df['text'].apply(lambda x: len(x) - x.count(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_of_punc(text):\n",
    "    temp = sum([1 for i in text if i in string.punctuation])\n",
    "    return round(temp/(len(text)- text.count(' ') )*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['punc_per'] = df['text'].apply(lambda x: percentage_of_punc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_punc</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_nonSW</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>len_text</th>\n",
       "      <th>punc_per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive been searching for the right words to than...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather...</td>\n",
       "      <td>[ive, searching, right, word, thank, breather,...</td>\n",
       "      <td>ive searching right word thank breather promis...</td>\n",
       "      <td>160</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "      <td>49</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid,...</td>\n",
       "      <td>even brother like speak treat like aid patent</td>\n",
       "      <td>62</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>date sunday</td>\n",
       "      <td>28</td>\n",
       "      <td>7.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                           text_punc  \\\n",
       "0  ive been searching for the right words to than...   \n",
       "1  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "2  nah i dont think he goes to usf he lives aroun...   \n",
       "3  even my brother is not like to speak with me t...   \n",
       "4                  i have a date on sunday with will   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words,...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "2  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                          text_nonSW  \\\n",
       "0  [ive, searching, right, words, thank, breather...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, goes, usf, lives, around, t...   \n",
       "3  [even, brother, like, speak, treat, like, aids...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                     text_lemmatized  \\\n",
       "0  [ive, searching, right, word, thank, breather,...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, go, usf, life, around, though]   \n",
       "3  [even, brother, like, speak, treat, like, aid,...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                          clean_text  len_text  punc_per  \n",
       "0  ive searching right word thank breather promis...       160      2.50  \n",
       "1  free entry 2 wkly comp win fa cup final tkts 2...       128      4.69  \n",
       "2           nah dont think go usf life around though        49      4.08  \n",
       "3      even brother like speak treat like aid patent        62      3.23  \n",
       "4                                        date sunday        28      7.14  "
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bedoe\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFSRJREFUeJzt3X+M3PWd3/Hn2z+wkxZMz7gRsYFdCpxsZwUExyYqnGQlOHYScC5AY1p0toKCLsVpAZGAG4W45K4JRDn3KlCu5IyOIC74ShLFEb5wSU3StALiNeCzNxxhgb2yZ0ocg3wEMHjNu3/M19Z4b3/MendndvfzfEir/c7n+/nuvOc7s6/5zme+85nITCRJZZjW6gIkSc1j6EtSQQx9SSqIoS9JBTH0Jakghr4kFcTQl6SCGPqSVBBDX5IKMqPVBfR3yimnZFtbW6vLkKRJZefOnb/JzHnD9Ztwod/W1kZnZ2ery5CkSSUi/r6Rfg7vSFJBDH1JKoihL0kFmXBj+pLUiEOHDtHb28vBgwdbXUpTzZ49mwULFjBz5szj2t7QlzQp9fb2cuKJJ9LW1kZEtLqcpshM9u/fT29vL+3t7cf1NxzekTQpHTx4kLlz5xYT+AARwdy5c0f16sbQlzRplRT4R4z2Nhv6klQQx/QlTQmbfvyrMf17N1xyzrB9enp6+PjHP86ePXvG9LrHk6EvTVWPfHXwdcs3NK8OTSgO70jSKBw+fJjPfOYzLF68mBUrVvDmm2/yrW99iw984AOce+65XH755bzxxhsArFu3js9+9rMsX76cM888k5/97Gd8+tOfZuHChaxbt64p9Rr6kjQKzz77LNdddx1dXV2cfPLJfPe73+WTn/wkO3bsYNeuXSxcuJDNmzcf7f/qq6+yfft2Nm3axKWXXsoNN9xAV1cXu3fv5qmnnhr3eg19SRqF9vZ2zjvvPAAuuOACenp62LNnDxdffDEdHR3cf//9dHV1He1/6aWXEhF0dHTwnve8h46ODqZNm8bixYvp6ekZ93oNfUkahVmzZh1dnj59On19faxbt44777yT3bt38+Uvf/mY8+qP9J82bdox206bNo2+vr5xr9fQl6Qx9tprr3Hqqady6NAh7r///laXcwzP3pE0JTRyimWzfOUrX2HZsmWcccYZdHR08Nprr7W6pKMiM1tdwzGWLFmSfomKNAam+CmbTz/9NAsXLmx1GS0x0G2PiJ2ZuWS4bR3ekaSCGPqSVBBDX5IK0lDoR8TKiHgmIroj4pYB1s+KiC3V+scjoq3f+tMj4rcRcdPYlC1JOh7Dhn5ETAfuAlYBi4CrImJRv27XAK9m5lnAJuD2fus3AX89+nIlSaPRyJH+UqA7M5/PzLeBB4DV/fqsBu6tlh8EPhTVpM8R8QngeaALSVJLNXKe/nzgxbrLvcCywfpkZl9EHADmRsSbwM3AJYBDO5LGz1CnqB6PKXBa60AaOdIf6Gta+p/cP1if/wxsyszfDnkFEddGRGdEdO7bt6+BkiRJx6OR0O8FTqu7vADYO1ifiJgBzAFeofaK4I6I6AGuB/5TRKzvfwWZeXdmLsnMJfPmzRvxjZCkZnv99df52Mc+xrnnnsv73vc+tmzZQltbGzfffDNLly5l6dKldHd3A/DDH/6QZcuWcf755/PhD3+Yl19+GYCNGzeydu1aVqxYQVtbG9/73vf4whe+QEdHBytXruTQoUNjXncjob8DODsi2iPiBGANsLVfn63A2mr5CmB71lycmW2Z2Qb8V+C/ZOadY1S7JLXMj370I9773veya9cu9uzZw8qVKwE46aST+MUvfsH69eu5/vrrAbjooot47LHHePLJJ1mzZg133HHH0b/z3HPP8dBDD/GDH/yAq6++muXLl7N7927e9a538dBDD4153cOGfmb2AeuBh4Gngb/KzK6IuC0iLqu6baY2ht8N3Aj8k9M6JWkq6ejo4Cc/+Qk333wzP//5z5kzZw4AV1111dHfjz76KAC9vb185CMfoaOjg69//evHTLW8atUqZs6cSUdHB4cPHz765NHR0TEuUy03NOFaZm4DtvVru7Vu+SBw5TB/Y+Nx1CdJE9I555zDzp072bZtGxs2bGDFihUAVCcuHrP8uc99jhtvvJHLLruMn/70p2zcuPFon/qplmfOnHl0m/GaatlP5ErScdi7dy/vfve7ufrqq7npppt44oknANiyZcvR3x/84AcBOHDgAPPnzwfg3nvvHfgPNolTK0uaGpp8iuXu3bv5/Oc/f/QI/Zvf/CZXXHEFb731FsuWLeOdd97hO9/5DlB7w/bKK69k/vz5XHjhhbzwwgtNrbWeUytLU5VTKzddW1sbnZ2dnHLKKeN6PU6tLElqiMM7kjRGmvHF5qPlkb6kSWuiDU83w2hvs6EvaVKaPXs2+/fvLyr4M5P9+/cze/bs4/4bDu9ImpQWLFhAb28vpc3XNXv2bBYsWHDc2xv6kialmTNn0t7e3uoyJh2HdySpIIa+JBXE0Jekghj6klQQQ1+SCmLoS1JBDH1JKoihL0kFMfQlqSCGviQVxNCXpIIY+pJUEENfkgpi6EtSQQx9SSqIoS9JBTH0Jakghr4kFcTQl6SCGPqSVBBDX5IKYuhLUkEMfUkqiKEvSQUx9CWpIIa+JBXE0Jekghj6klQQQ1+SCmLoS1JBGgr9iFgZEc9ERHdE3DLA+lkRsaVa/3hEtFXtSyPiqepnV0T8/tiWL0kaiWFDPyKmA3cBq4BFwFURsahft2uAVzPzLGATcHvVvgdYkpnnASuB/x4RM8aqeEnSyDQSwEuB7sx8HiAiHgBWA7+s67Ma2FgtPwjcGRGRmW/U9ZkN5KgrllTzyFdbXYEmoUaGd+YDL9Zd7q3aBuyTmX3AAWAuQEQsi4guYDfwh9V6SVILNBL6MUBb/yP2Qftk5uOZuRj4ALAhImb/kyuIuDYiOiOic9++fQ2UJEk6Ho2Efi9wWt3lBcDewfpUY/ZzgFfqO2Tm08DrwPv6X0Fm3p2ZSzJzybx58xqvXpI0Io2E/g7g7Ihoj4gTgDXA1n59tgJrq+UrgO2ZmdU2MwAi4gzgd4GeMalckjRiw76Rm5l9EbEeeBiYDtyTmV0RcRvQmZlbgc3AfRHRTe0If021+UXALRFxCHgH+PeZ+ZvxuCGSpOE1dPpkZm4DtvVru7Vu+SBw5QDb3QfcN8oaJUljxE/kSlJBDH1JKoihL0kFMfQlqSCGviQVxNCXpIIY+pJUEENfkgpi6EtSQQx9SSqIoS9JBTH0Jakghr4kFcTQl6SCGPqSVBBDX5IKYuhLUkEMfUkqiKEvSQUx9CWpIIa+JBXE0Jekghj6klQQQ1+SCmLoS1JBDH1JKoihL0kFMfQlqSCGviQVxNCXpIIY+pJUkBmtLkDSIB75aqsr0BTkkb4kFcTQl6SCGPqSVBBDX5IKYuhLUkEMfUkqiKEvSQVp6Dz9iFgJ/CkwHfjzzPxav/WzgG8DFwD7gU9lZk9EXAJ8DTgBeBv4fGZuH8P6J5RNP/7VkOtvuOScJlUiSQMb9kg/IqYDdwGrgEXAVRGxqF+3a4BXM/MsYBNwe9X+G+DSzOwA1gL3jVXhkqSRa2R4ZynQnZnPZ+bbwAPA6n59VgP3VssPAh+KiMjMJzNzb9XeBcyuXhVIklqgkdCfD7xYd7m3ahuwT2b2AQeAuf36XA48mZlvHV+pkqTRamRMPwZoy5H0iYjF1IZ8Vgx4BRHXAtcCnH766Q2UJEk6Ho0c6fcCp9VdXgDsHaxPRMwA5gCvVJcXAN8H/iAznxvoCjLz7sxckplL5s2bN7JbIElqWCNH+juAsyOiHfgHYA3wb/v12UrtjdpHgSuA7ZmZEXEy8BCwITP/z9iVPTkNdXaPZ/ZIaoZhQz8z+yJiPfAwtVM278nMroi4DejMzK3AZuC+iOimdoS/ptp8PXAW8KWI+FLVtiIzfz3WN0TSCAw3bfPyDc2pQ03X0Hn6mbkN2Nav7da65YPAlQNs90fAH42yRknSGPETuZJUEENfkgpi6EtSQQx9SSqIoS9JBTH0Jakghr4kFaSh8/Q1/pyLX1IzGPqThE8KksaCwzuSVBBDX5IKYuhLUkEMfUkqiKEvSQUx9CWpIIa+JBXE0JekgvjhrBEY7gNSkjTRGfpSqwz3PbXSOHB4R5IKYuhLUkEMfUkqiKEvSQUx9CWpIIa+JBXE0Jekghj6klQQQ1+SCmLoS1JBnIZBGi9Os6AJyCN9SSqIoS9JBTH0Jakghr4kFcTQl6SCGPqSVBBDX5IK4nn6U8Rw3997wyXnNKkSSROZR/qSVJCGQj8iVkbEMxHRHRG3DLB+VkRsqdY/HhFtVfvciHgkIn4bEXeObemSpJEaNvQjYjpwF7AKWARcFRGL+nW7Bng1M88CNgG3V+0HgS8BN41ZxZKk49bIkf5SoDszn8/Mt4EHgNX9+qwG7q2WHwQ+FBGRma9n5v+mFv6SpBZrJPTnAy/WXe6t2gbsk5l9wAFg7lgUKEkaO42EfgzQlsfRZ/AriLg2IjojonPfvn2NbiZJGqFGQr8XOK3u8gJg72B9ImIGMAd4pdEiMvPuzFySmUvmzZvX6GaSpBFqJPR3AGdHRHtEnACsAbb267MVWFstXwFsz8yGj/QlSc0x7IezMrMvItYDDwPTgXsysysibgM6M3MrsBm4LyK6qR3hrzmyfUT0ACcBJ0TEJ4AVmfnLsb8pkqThNPSJ3MzcBmzr13Zr3fJB4MpBtm0bRX2SpDHkNAyFcJoGSeA0DJJUFENfkgpi6EtSQRzTFzD8mP9QfD9Amjw80pekghj6klQQQ1+SCmLoS1JBDH1JKoihL0kF8ZRNjZpTPEiTh0f6klQQj/SloTzy1aHXL9/QnDqkMWLoS6Mx3JOCNME4vCNJBfFIX+OupW/0OjwjHcMjfUkqiKEvSQVxeEct53n+UvMY+iqbZ9+oMIa+NEE9+vz+Idd/8My5TapEU4mhrwlvqOEfh36kkTH0pUlqXF8JeKrrlOXZO5JUEI/0NawL/+/dQ65/7PRrm1TJAIY5InVcXDqWoa9JbbhQH+32wz0pDLX9aLaVxouhLw1hNMFsqGsickxfkgpi6EtSQRzeqTPcdACSNNl5pC9JBfFIX5qiRnNmkaYuQ3+SmNDnyo+z4W67pMYVFfqO2U9MhrrUPEWFvsaHoV0g5+aZtAx9qUBOT1GuKRf6pQ7hjHbM36N1qQxTLvQnsqGCdSq/EaupZ9hXCsubVIhGrKHQj4iVwJ8C04E/z8yv9Vs/C/g2cAGwH/hUZvZU6zYA1wCHgf+QmQ+PWfVTiEfamlIc85+whg39iJgO3AVcAvQCOyJia2b+sq7bNcCrmXlWRKwBbgc+FRGLgDXAYuC9wE8i4pzMPDzWN6QZSj5tUmVp5WRxww3R+m1po9PIkf5SoDsznweIiAeA1UB96K8GNlbLDwJ3RkRU7Q9k5lvACxHRXf29R8emfEkT0XBPGo/1Tc333ibDE1YjoT8feLHuci+wbLA+mdkXEQeAuVX7Y/22nX/c1Y6zqTzEMpVvmyaf0by/NVywtvIV+XDXvenHQ193M54UGgn9GKAtG+zTyLZExLXAkb3x24h4poG6BnMK8JtRbD8K3xhqZQvrGpJ1jYx1jcxx1DXk/9EY+Aa0bH8NfdtuHF1dZzTSqZHQ7wVOq7u8ANg7SJ/eiJgBzAFeaXBbMvNuYEwORSOiMzOXjMXfGkvWNTLWNTLWNTIl19XILJs7gLMjoj0iTqD2xuzWfn22Amur5SuA7ZmZVfuaiJgVEe3A2cAvxqZ0SdJIDXukX43RrwcepnbK5j2Z2RURtwGdmbkV2AzcV71R+wq1Jwaqfn9F7U3fPuC6yXrmjiRNBQ2dp5+Z24Bt/dpurVs+CFw5yLZ/DPzxKGocqYn6jqV1jYx1jYx1jUyxdUVtFEaSVAK/OUuSCjJlQj8iVkbEMxHRHRG3tLCO0yLikYh4OiK6IuI/Vu0bI+IfIuKp6uejLaitJyJ2V9ffWbX9TkT8OCKerX7/iybX9Lt1++SpiPjHiLi+FfsrIu6JiF9HxJ66tgH3T9T8t+rx9rcR8f4m1/X1iPi76rq/HxEnV+1tEfFm3X77sybXNej9FhEbqv31TER8pMl1bamrqScinqram7m/BsuG5j7GMnPS/1B7g/k54EzgBGAXsKhFtZwKvL9aPhH4FbCI2ieWb2rxfuoBTunXdgdwS7V8C3B7i+/H/0ftfOOm7y/g94D3A3uG2z/AR4G/pvZZlAuBx5tc1wpgRrV8e11dbfX9WrC/Brzfqv+BXcAsoL36f53erLr6rf8GcGsL9tdg2dDUx9hUOdI/OlVEZr4NHJkqouky86XMfKJafg14mgn8KWRq++neavle4BMtrOVDwHOZ+fetuPLM/F/Uzj6rN9j+WQ18O2seA06OiFObVVdm/k1m9lUXH6P2GZimGmR/DebolCyZ+QJwZEqWptYVEQH8G+A743HdQxkiG5r6GJsqoT/QVBEtD9qIaAPOBx6vmtZXL9PuafYwSiWBv4mInVH7FDTAezLzJag9KIF/2YK6jljDsf+Mrd5fMPj+mUiPuU9TOyI8oj0inoyIn0XExS2oZ6D7baLsr4uBlzPz2bq2pu+vftnQ1MfYVAn9hqZ7aKaI+OfAd4HrM/MfgW8C/wo4D3iJ8f+s+UD+dWa+H1gFXBcRv9eCGgYUtQ/+XQb8j6ppIuyvoUyIx1xEfJHaZ2Dur5peAk7PzPOBG4G/jIiTmljSYPfbhNhfwFUce2DR9P01QDYM2nWAtlHvs6kS+g1N99AsETGT2p16f2Z+DyAzX87Mw5n5DvAtxuml7VAyc2/1+9fA96saXj7ykrH6/etm11VZBTyRmS9XNbZ8f1UG2z8tf8xFxFrg48C/y2oQuBo+2V8t76Q2dt60qR2HuN8mwv6aAXwS2HKkrdn7a6BsoMmPsakS+o1MFdEU1ZjhZuDpzPyTuvb6sbjfB/b033ac6/pnEXHikWVqbwTu4dgpNNYCP2hmXXWOOQJr9f6qM9j+2Qr8QXWGxYXAgSMv0Zshal9sdDNwWWa+Udc+L2rfgUFEnElt6pPnm1jXYPfbRJiS5cPA32Vm75GGZu6vwbKBZj/GmvGudTN+qL3T/Stqz9RfbGEdF1F7Cfa3wFPVz0eB+4DdVftW4NQm13UmtbMndgFdR/YRtSmw/yfwbPX7d1qwz95N7RvX5tS1NX1/UXvSeQk4RO0o65rB9g+1l953VY+33cCSJtfVTW2898hj7M+qvpdX9+8u4Ang0ibXNej9Bnyx2l/PAKuaWVfV/hfAH/br28z9NVg2NPUx5idyJakgU2V4R5LUAENfkgpi6EtSQQx9SSqIoS9JBTH0Jakghr4kFcTQl6SC/H+3JJc1BsjXIgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0,200,40)\n",
    "plt.hist(df[df['label'] == 'ham']['len_text'], bins=bins, label='ham', alpha = 0.5, normed=True)\n",
    "plt.hist(df[df['label'] == 'spam']['len_text'], bins=bins, label='spam', alpha = 0.5, normed=True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bedoe\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0,50,40)\n",
    "plt.hist(df[df['label'] == 'ham']['punc_per'], bins=bins, label='ham', alpha = 0.5, normed=True)\n",
    "plt.hist(df[df['label'] == 'spam']['punc_per'], bins=bins, label='spam', alpha = 0.5, normed=True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97396768, 0.97755835, 0.97396768, 0.95867026, 0.97394429])"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = RandomForestClassifier()\n",
    "k_fold = KFold(n_splits=5)\n",
    "cross_val_score(cls, df_Tf_idf, df['label'], cv=k_fold, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
